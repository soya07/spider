{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding='utf-8'\n",
    "#百度贴吧爬虫 获取每个帖子的用户ID,评论内容，时间\n",
    "#路径 /DB 分页内容     /FILES URL数据表\n",
    "#cfy2400@foxmail.com 方\n",
    "#PS:仅适用新版贴吧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import xlwt\n",
    "import xlrd\n",
    "import requests\n",
    "import time\n",
    "import math\n",
    "import queue\n",
    "import threading\n",
    "import json\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class spyThread(threading.Thread):\n",
    "    '''\n",
    "        -level 内容详细程度\n",
    "        -queue 队列\n",
    "        -number 爬虫线程数\n",
    "        -check 爬取链接或页面内容\n",
    "        -url 要爬取的链接地址\n",
    "        -name 数据保存命名\n",
    "    '''\n",
    "    def __init__(self,level,queue,queue1,number,check,url,name):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.l=level\n",
    "        self.q=queue\n",
    "        self.q1=queue1\n",
    "        self.c=check\n",
    "        self.u=url\n",
    "        self.name=name\n",
    "        try:\n",
    "            self.n=number\n",
    "        except:\n",
    "            self.n=-1\n",
    "\n",
    "    def run(self):\n",
    "        count=0\n",
    "        if self.c=='save_content':\n",
    "            #从excel表中依次读取URL分别访问，将每个页面的内容分别保存至excel\n",
    "            while not self.q.empty():\n",
    "                url=self.q.get()\n",
    "                url_1=url+'?pn='#拼接参数pn\n",
    "                book=xlwt.Workbook()#新建一个excel文件\n",
    "                sheet=book.add_sheet('sheet1')#新建一个工作簿\n",
    "                urlf=url_1+'1'#第一次建立链接的地址\n",
    "                res=requests.get(urlf)#建立第一次URL连接\n",
    "                soup=BeautifulSoup(res.text,'html.parser')#将网页存入soup\n",
    "                #try :\n",
    "                try:\n",
    "                    tit=soup.findAll('h3',('core_title_txt'))[0].text#抓取标题\n",
    "                except:\n",
    "                    tit=soup.findAll('h1',('core_title_txt'))[0].text#抓取标题\n",
    "                max_page=soup.findAll('span',('red'))[1].text#获取最大页数\n",
    "                for j in range(1,int(max_page)+1):\n",
    "                    url=url_1+str(j)#循环赋值URL抓取多页数据\n",
    "                    res1=requests.get(url)#建立URL连接\n",
    "                    soup1=BeautifulSoup(res1.text,'html.parser')#将网页存入soup\n",
    "                    data=get_data(soup1)#爬取数据\n",
    "                    save_xl(data,j,sheet,tit,urlf)#写入excel\n",
    "                if os.path.exists('./db/'+self.name)==0:\n",
    "                    os.mkdir('./db/'+self.name)\n",
    "\n",
    "                road=os.getcwd()+'\\\\db\\\\'+self.name+'\\\\'+validateTitle(tit)+'.xls'\n",
    "                book.save(road)#保存文件\n",
    "                #print(road+'【保存完毕】')\n",
    "                #except:\n",
    "                    #print ('页面',url,'抓取错误')\n",
    "        elif self.c=='save_url':\n",
    "            #将主链接下的所有子链接存入队列\n",
    "            while not self.q.empty():\n",
    "                url=self.u\n",
    "                pn=self.q.get()\n",
    "                url1=url+str(pn*50)\n",
    "                #print(url1)\n",
    "                try:\n",
    "                    soup=get_html(url1)\n",
    "                    rs=soup.findAll('a',{'class':'j_th_tit'})\n",
    "                    for k in rs:\n",
    "                        \n",
    "                        href='http://tieba.baidu.com/'+k.get('href')\n",
    "                        tit=k.text\n",
    "                        self.q1.put([tit,href])\n",
    "                    #print('第%d页爬取完成'%pn)\n",
    "                except:\n",
    "                    print (url1,'数据写入错误')\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def get_result(self):\n",
    "        return self.q1\n",
    "def validateTitle(title):\n",
    "    #转换标题中的非法字符\n",
    "    rstr = r\"[\\/\\\\\\:\\*\\?\\\"\\<\\>\\|]\"  # '/ \\ : * ? \" < > |'\n",
    "    new_title = re.sub(rstr, \"_\", title)  # 替换为下划线\n",
    "    return new_title\n",
    "\n",
    "def get_data(soup):\n",
    "    #获取数据保存在字典中\n",
    "    name=[]\n",
    "    tim=[]\n",
    "    content=[]\n",
    "    item=[]\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    count=len(soup.findAll('div',{'j_l_post'}))\n",
    "    for i in range(0,count):#提取时间\n",
    "\n",
    "        t=json.loads(soup.findAll('div',{'j_l_post'})[i].get('data-field'))['content']['date']#时间\n",
    "        print(t)\n",
    "        tim.append(t)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        count=len(soup.findAll('div',{'j_l_post'}))\n",
    "        for i in range(0,count):#提取时间\n",
    "            t=json.loads(soup.findAll('div',{'j_l_post'})[i].get('data-field'))['content']['date']#时间\n",
    "            tim.append(t)\n",
    "            #print('A:',t)\n",
    "    except:\n",
    "        times=soup.findAll('span',('tail-info'))#时间\n",
    "        for i in range(0,len(times)):#提取时间\n",
    "            t=(times[i].text)\n",
    "            try:#判断多个相同标签下哪个字段是时间\n",
    "                if \":\" in t:\n",
    "                    time.strptime(t, \"%Y-%m-%d %H:%M\")\n",
    "                    tim.append(t)\n",
    "                #print('B:',t)\n",
    "            except:\n",
    "                return 0\n",
    "\n",
    "    names=soup.findAll('a',('p_author_name'))#用户名称\n",
    "    contents=soup.findAll('div',('d_post_content'))#内容\n",
    "    for i in range(0,len(names)):#提取用户名称\n",
    "        n=(names[i].text)\n",
    "        name.append(n)\n",
    "    for i in range(0,len(contents)):#提取评论内容\n",
    "        c=(contents[i].text.strip())\n",
    "        c=c.rstrip()\n",
    "        content.append(c)\n",
    "    #print(len(name),len(tim),len(content))\n",
    "    for i in range(0,len(name)):#将列表转换为字典\n",
    "        dic={}\n",
    "        dic.update(name=name[i],time=tim[i],content=content[i])\n",
    "        item.append(dic)\n",
    "    return item\n",
    "\n",
    "def spy_xlurl_to_xl(name,maxpage):\n",
    "    #将现有excel中的链接批量访问抓取数据，并保存至excel中\n",
    "    url_q=queue.Queue()           #网址队列\n",
    "    threadCount=10                #线程数量\n",
    "    level=1                       #内容级别\n",
    "    maxpn=maxpage*50+1            #最大URL数量\n",
    "    check='save_content'\n",
    "    try:\n",
    "        workbook=xlrd.open_workbook('./files/'+name+'.xls')\n",
    "    except:\n",
    "        print('未找到表')\n",
    "        return False\n",
    "    f=workbook.sheet_by_index(0)\n",
    "    trows=f.nrows\n",
    "    time1=time.time()\n",
    "    if trows:\n",
    "        if maxpn>trows:\n",
    "            maxpn=trows\n",
    "        #print(trows,maxpn)\n",
    "        for i in range(0,maxpn):     #控制最大URL数量\n",
    "            url_q.put(f.cell(i,1).value)   #将URL放进队列\n",
    "        for i in range(threadCount):  \n",
    "            t=spyThread(level,url_q,'',i,check,'',name)\n",
    "            t.setDaemon(True)\n",
    "            t.start()\n",
    "            #print ('线程%d开始'%i)\n",
    "        print('爬取数据中...')\n",
    "        for i in range(threadCount):\n",
    "            t.join()\n",
    "            #print ('线程%d结束'%i)\n",
    "        print('爬虫工作结束，共用时:',time.time()-time1,'s')\n",
    "        print ('Done')\n",
    "    else:\n",
    "        print('未获取到有效URL')\n",
    "    \n",
    "def save_xl(data,page,sheet,tit,url):\n",
    "    #接收数据.当前页数.工作表\n",
    "    minlines=(page-1)*30+1\n",
    "    maxlines=(page-1)*30+len(data)+1\n",
    "    j=0   #控制data数组下标\n",
    "    if page==1:\n",
    "        #当页面为第一页时填写表头\n",
    "        sheet.write(0,0,'序号')\n",
    "        sheet.write(0,1,'用户ID')\n",
    "        sheet.write(0,2,'时间')\n",
    "        sheet.write(0,3,'内容')\n",
    "        sheet.write(0,4,url)\n",
    "    for i in range(minlines,maxlines):\n",
    "        #依次写入数据到excel\n",
    "        sheet.write(i,0,i)\n",
    "        sheet.write(i,1,data[j]['name'])\n",
    "        sheet.write(i,2,data[j]['time'])\n",
    "        sheet.write(i,3,data[j]['content'])\n",
    "        j+=1\n",
    "        \n",
    "def save_url(q,name):\n",
    "    #将爬取到的URL存入excel\n",
    "    book=xlwt.Workbook()\n",
    "    sheet=book.add_sheet('sheet1',cell_overwrite_ok=True)\n",
    "    j=0\n",
    "    while not q.empty():\n",
    "        ar=q.get()\n",
    "        tit=ar[0]\n",
    "        href=ar[1]\n",
    "        sheet.write(j,0,tit)\n",
    "        sheet.write(j,1,href)\n",
    "        j+=1\n",
    "        #print(tit+' 【完毕】')\n",
    "    book.save('./files/'+name+'.xls')\n",
    "    return j\n",
    "        \n",
    "def get_html(url):\n",
    "    #请求URL,返回HTML页面\n",
    "    res=requests.get(url)\n",
    "    soup=BeautifulSoup(res.text,\"html.parser\")\n",
    "    res.close()\n",
    "    return soup\n",
    "\n",
    "def spy_url_save(url,name,maxpage):\n",
    "    #爬取所有子链接到表\n",
    "    q=queue.Queue()               #页面数队列\n",
    "    q1=queue.Queue()              #临时存放URL\n",
    "    threadCount=10                #线程数量\n",
    "    level=1                       #内容级别\n",
    "    check='save_url'\n",
    "    time1=time.time()\n",
    "    for i in range(0,maxpage):     #控制最大页面数量\n",
    "        q.put(i)\n",
    "    for i in range(threadCount):  \n",
    "        t=spyThread(level,q,q1,i,check,url,name)\n",
    "        t.setDaemon(True)\n",
    "        t.start()\n",
    "        #print ('线程%d开始'%i)\n",
    "    print('爬取数据中...')\n",
    "    for i in range(threadCount):\n",
    "        t.join()\n",
    "        #print ('线程%d结束'%i)\n",
    "    print('爬取完毕，写入excel数据中...')\n",
    "    count=save_url(q1,name)\n",
    "    print ('已将所有URL写入excel')\n",
    "    print('爬虫工作结束，共计',count,'条数据，共用时:',time.time()-time1,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "爬取数据中...\n",
      "爬虫工作结束，共用时: 24.249629259109497 s\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    #start=1 爬取链接 start=2 分页爬取页面内容\n",
    "    start=2\n",
    "\n",
    "    name='复仇者联盟'\n",
    "    url='https://tieba.baidu.com/f?kw='+name+'&ie=utf-8&pn='\n",
    "    if start==1:\n",
    "        #需要爬取的页面数，每页50个子链接\n",
    "        maxpage=10\n",
    "        spy_url_save(url,name,maxpage)#爬取主页面下的子链接存入excel\n",
    "    elif start==2:\n",
    "        #需要爬取的页面数，每页50个子链接\n",
    "        maxpage=1\n",
    "        spy_xlurl_to_xl(name,maxpage)#将excel中的链接读取分别访问爬取内容，并单独保存为excel\n",
    "    else:\n",
    "        print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
